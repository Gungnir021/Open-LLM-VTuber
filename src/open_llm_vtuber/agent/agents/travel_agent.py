import os
import sys
import json
import requests
from typing import AsyncIterator, List, Dict, Any, Callable, Literal
from loguru import logger
from dotenv import load_dotenv

from .agent_interface import AgentInterface
from ..output_types import SentenceOutput, DisplayText
from ..input_types import BatchInput, TextSource, ImageSource
from ...chat_history_manager import get_history
from ..transformers import (
    sentence_divider,
    actions_extractor,
    tts_filter,
    display_processor,
)
from ...config_manager import TTSPreprocessorConfig
from .tools.get_weather import get_weather

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. è¯»å–ç¯å¢ƒå˜é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 
load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
AMAP_API_KEY = os.getenv("AMAP_API_KEY")

if not DEEPSEEK_API_KEY:
    logger.warning("âŒ æœªæ£€æµ‹åˆ° DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ã€‚")
if not AMAP_API_KEY:
    logger.warning("âŒ æœªæ£€æµ‹åˆ° AMAP_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ã€‚")

from ..stateless_llm.stateless_llm_interface import StatelessLLMInterface

class TravelAgent(AgentInterface):
    """
    æ—…è¡ŒåŠ©æ‰‹ Agentï¼Œæ”¯æŒ DeepSeek Function Calling
    å®ç°å¤©æ°”æŸ¥è¯¢ã€æ—…è¡Œå»ºè®®ç­‰åŠŸèƒ½
    """

    _system: str = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡ŒåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å¤©æ°”ã€æä¾›æ—…è¡Œå»ºè®®ã€‚
å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ï¼Œè¯·ä½¿ç”¨ get_weather å‡½æ•°è·å–å‡†ç¡®çš„å¤©æ°”ä¿¡æ¯ã€‚
è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚"""

    def __init__(
        self,
        llm: StatelessLLMInterface,
        system_prompt: str = None,
        live2d_model=None,
        tts_preprocessor_config: TTSPreprocessorConfig = None,
        faster_first_response: bool = True,
        segment_method: str = "pysbd",
        interrupt_method: Literal["system", "user"] = "user",
    ):
        """
        åˆå§‹åŒ–æ—…è¡ŒåŠ©æ‰‹
        
        Args:
            llm: StatelessLLMInterface - LLM å®ä¾‹
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            live2d_model: Live2D æ¨¡å‹
            tts_preprocessor_config: TTS é¢„å¤„ç†é…ç½®
            faster_first_response: æ˜¯å¦å¯ç”¨å¿«é€Ÿé¦–æ¬¡å“åº”
            segment_method: å¥å­åˆ†å‰²æ–¹æ³•
            interrupt_method: ä¸­æ–­å¤„ç†æ–¹æ³•
        """
        super().__init__()
        self._memory = []
        self._llm = llm
        self._live2d_model = live2d_model
        self._tts_preprocessor_config = tts_preprocessor_config
        self._faster_first_response = faster_first_response
        self._segment_method = segment_method
        self.interrupt_method = interrupt_method
        self._interrupt_handled = False
        
        if system_prompt:
            self._system = system_prompt
        
        # è®¾ç½®èŠå¤©åŠŸèƒ½
        self.chat = self._chat_function_factory(llm.chat_completion)
        logger.info("TravelAgent initialized.")

    def _set_llm(self, llm: StatelessLLMInterface):
        """
        è®¾ç½®è¦ä½¿ç”¨çš„ LLM
        
        Args:
            llm: StatelessLLMInterface - LLM å®ä¾‹
        """
        self._llm = llm
        self.chat = self._chat_function_factory(llm.chat_completion)

    def _add_message(
        self,
        message: str | List[Dict[str, Any]],
        role: str,
        display_text: DisplayText | None = None,
    ):
        """æ·»åŠ æ¶ˆæ¯åˆ°è®°å¿†ä¸­"""
        if isinstance(message, list):
            text_content = ""
            for item in message:
                if item.get("type") == "text":
                    text_content += item["text"]
        else:
            text_content = message

        message_data = {
            "role": role,
            "content": text_content,
        }

        if display_text:
            if display_text.name:
                message_data["name"] = display_text.name
            if display_text.avatar:
                message_data["avatar"] = display_text.avatar

        self._memory.append(message_data)

    def set_memory_from_history(self, conf_uid: str, history_uid: str) -> None:
        """ä»èŠå¤©å†å²åŠ è½½è®°å¿†"""
        messages = get_history(conf_uid, history_uid)

        self._memory = []
        self._memory.append({
            "role": "system",
            "content": self._system,
        })

        for msg in messages:
            self._memory.append({
                "role": "user" if msg["role"] == "human" else "assistant",
                "content": msg["content"],
            })

    def handle_interrupt(self, heard_response: str) -> None:
        """å¤„ç†ç”¨æˆ·ä¸­æ–­"""
        if self._interrupt_handled:
            return

        self._interrupt_handled = True

        if self._memory and self._memory[-1]["role"] == "assistant":
            self._memory[-1]["content"] = heard_response + "..."
        else:
            if heard_response:
                self._memory.append({
                    "role": "assistant",
                    "content": heard_response + "...",
                })
            self._memory.append({
                "role": "system" if self.interrupt_method == "system" else "user",
                "content": "[Interrupted by user]",
            })

    def reset_interrupt(self) -> None:
        """é‡ç½®ä¸­æ–­æ ‡å¿—"""
        self._interrupt_handled = False

    def _to_text_prompt(self, input_data: BatchInput) -> str:
        """å°† BatchInput æ ¼å¼åŒ–ä¸ºæç¤ºå­—ç¬¦ä¸²"""
        message_parts = []

        for text_data in input_data.texts:
            if text_data.source == TextSource.INPUT:
                message_parts.append(text_data.content)
            elif text_data.source == TextSource.CLIPBOARD:
                message_parts.append(f"[Clipboard content: {text_data.content}]")

        if input_data.images:
            message_parts.append("\nImages in this message:")
            for i, img_data in enumerate(input_data.images, 1):
                source_desc = {
                    ImageSource.CAMERA: "captured from camera",
                    ImageSource.SCREEN: "screenshot",
                    ImageSource.CLIPBOARD: "from clipboard",
                    ImageSource.UPLOAD: "uploaded",
                }[img_data.source]
                message_parts.append(f"- Image {i} ({source_desc})")

        return "\n".join(message_parts)

    def _deepseek_function_call(self, query: str) -> str:
        """ä½¿ç”¨ DeepSeek API è¿›è¡Œå‡½æ•°è°ƒç”¨"""
        print("\nğŸ”§ [DEBUG] å¼€å§‹å°è¯• DeepSeek Function Calling...")
        print(f"ğŸ”§ [DEBUG] ç”¨æˆ·è¾“å…¥: {query}")
        
        if not DEEPSEEK_API_KEY:
            print("âŒ [DEBUG] DeepSeek API Key æœªé…ç½®")
            return "âŒ DeepSeek API Key æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨æ™ºèƒ½åŠŸèƒ½ã€‚"
            
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºåŒ…å«è®°å¿†çš„æ¶ˆæ¯åˆ—è¡¨
        messages = self._memory.copy() if self._memory else []
        
        # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
        if not messages or messages[0]["role"] != "system":
            messages.insert(0, {
                "role": "system",
                "content": self._system
            })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": query})
        
        print(f"ğŸ”§ [DEBUG] æ„å»ºçš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # æ­¥éª¤1: é¦–æ¬¡è°ƒç”¨è·å–å‡½æ•°è°ƒç”¨è¯·æ±‚
        payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "tools": [{
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "åŸå¸‚åç§°ï¼Œå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·ç­‰"
                            }
                        },
                        "required": ["location"]
                    }
                }
            }],
            "tool_choice": "auto"  # è®© AI è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
        }
        
        try:
            print("ğŸ”§ [DEBUG] æ­£åœ¨è°ƒç”¨ DeepSeek API...")
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            ).json()
            
            print(f"ğŸ”§ [DEBUG] DeepSeek API å“åº”çŠ¶æ€: æˆåŠŸ")
            
            # æ£€æŸ¥æ˜¯å¦è¦æ±‚è°ƒç”¨å‡½æ•°
            tool_calls = response["choices"][0]["message"].get("tool_calls")
            if not tool_calls:
                # AI åˆ¤æ–­ä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¿”å›æ™®é€šå›å¤
                print("ğŸ”§ [DEBUG] AI åˆ¤æ–­ä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¿”å›æ™®é€šå›å¤")
                return response["choices"][0]["message"]["content"]
            
            print(f"ğŸ”§ [DEBUG] AI å†³å®šè°ƒç”¨å·¥å…·ï¼Œå·¥å…·æ•°é‡: {len(tool_calls)}")
            
            # æ­¥éª¤2: æ‰§è¡Œå‡½æ•°è°ƒç”¨
            function_name = tool_calls[0]["function"]["name"]
            function_args = json.loads(tool_calls[0]["function"]["arguments"])
            
            print(f"ğŸ”§ [DEBUG] è°ƒç”¨å‡½æ•°: {function_name}")
            print(f"ğŸ”§ [DEBUG] å‡½æ•°å‚æ•°: {function_args}")
            
            if function_name == "get_weather":
                print("ğŸŒ¤ï¸ [DEBUG] æ­£åœ¨æ‰§è¡Œå¤©æ°”æŸ¥è¯¢...")
                weather_result = get_weather(function_args["location"])
                print(f"ğŸŒ¤ï¸ [DEBUG] å¤©æ°”æŸ¥è¯¢ç»“æœ: {weather_result[:100]}...")
            else:
                print(f"âŒ [DEBUG] æœªçŸ¥å‡½æ•°è°ƒç”¨: {function_name}")
                weather_result = json.dumps({"error": "æœªçŸ¥å‡½æ•°è°ƒç”¨"}, ensure_ascii=False)
            
            # æ­¥éª¤3: å°†å‡½æ•°ç»“æœè¿”å›ç»™æ¨¡å‹
            print("ğŸ”§ [DEBUG] å°†å‡½æ•°ç»“æœè¿”å›ç»™ DeepSeek æ¨¡å‹...")
            payload["messages"].append(response["choices"][0]["message"])
            payload["messages"].append({
                "role": "tool",
                "content": weather_result,
                "tool_call_id": tool_calls[0]["id"]
            })
            
            final_response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            ).json()
            
            final_content = final_response["choices"][0]["message"]["content"]
            print(f"ğŸ”§ [DEBUG] Function Calling å®Œæˆï¼Œæœ€ç»ˆå›å¤é•¿åº¦: {len(final_content)}")
            print("âœ… [DEBUG] DeepSeek Function Calling æ‰§è¡ŒæˆåŠŸï¼")
            
            return final_content
            
        except Exception as e:
            print(f"âŒ [DEBUG] DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"DeepSeek API è°ƒç”¨å¤±è´¥: {str(e)}")
            return f"âŒ æŠ±æ­‰ï¼Œæ™ºèƒ½åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨: {str(e)}"

    def _chat_function_factory(
        self, chat_func: Callable[[List[Dict[str, Any]], str], AsyncIterator[str]]
    ) -> Callable[..., AsyncIterator[SentenceOutput]]:
        """
        åˆ›å»ºèŠå¤©ç®¡é“ï¼Œä¼˜å…ˆä½¿ç”¨ DeepSeek Function Calling
        
        ç®¡é“æµç¨‹:
        DeepSeek Function Calling -> sentence_divider -> actions_extractor -> display_processor -> tts_filter
        """
        
        @tts_filter(self._tts_preprocessor_config)
        @display_processor()
        @actions_extractor(self._live2d_model)
        @sentence_divider(
            faster_first_response=self._faster_first_response,
            segment_method=self._segment_method,
            valid_tags=["think"],
        )
        async def chat_with_memory(input_data: BatchInput) -> AsyncIterator[str]:
            """
            ä½¿ç”¨è®°å¿†å’Œå¤„ç†ç®¡é“çš„èŠå¤©å®ç°ï¼Œä¼˜å…ˆä½¿ç”¨ Function Calling
            
            Args:
                input_data: BatchInput
            
            Returns:
                AsyncIterator[str] - æ¥è‡ª LLM çš„ token æµ
            """
            
            user_input = self._to_text_prompt(input_data)
            print(f"\nğŸ’¬ [DEBUG] æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input}")
            
            # ä¼˜å…ˆå°è¯• DeepSeek Function Calling
            # è®© AI è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            print("ğŸš€ [DEBUG] å¼€å§‹å¤„ç†ç”¨æˆ·è¯·æ±‚...")
            try:
                print("ğŸ”§ [DEBUG] å°è¯•ä½¿ç”¨ DeepSeek Function Calling...")
                response = self._deepseek_function_call(user_input)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè°ƒç”¨äº†å‡½æ•°ï¼ˆé€šè¿‡å“åº”å†…å®¹åˆ¤æ–­ï¼‰
                if not response.startswith("âŒ"):
                    # æˆåŠŸä½¿ç”¨ Function Callingï¼Œæµå¼è¾“å‡ºå“åº”
                    print("âœ… [DEBUG] Function Calling æˆåŠŸï¼Œå¼€å§‹æµå¼è¾“å‡º...")
                    for char in response:
                        yield char
                    
                    # å­˜å‚¨åˆ°è®°å¿†
                    self._add_message(user_input, "user")
                    self._add_message(response, "assistant")
                    print("âœ… [DEBUG] å“åº”å·²å­˜å‚¨åˆ°è®°å¿†ä¸­")
                    return
                else:
                    # Function Calling å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­ä½¿ç”¨æ™®é€šèŠå¤©
                    print(f"âš ï¸ [DEBUG] Function calling ä¸å¯ç”¨: {response}")
                    logger.info(f"Function calling ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€šèŠå¤©æ¨¡å¼: {response}")
                    
            except Exception as e:
                print(f"âŒ [DEBUG] Function calling å‡ºé”™: {str(e)}")
                logger.error(f"Function calling å‡ºé”™ï¼Œå›é€€åˆ°æ™®é€šèŠå¤©: {str(e)}")
            
            # å›é€€åˆ°æ™®é€šèŠå¤©æµç¨‹
            print("ğŸ”„ [DEBUG] å›é€€åˆ°æ™®é€šèŠå¤©æµç¨‹...")
            messages = self._to_messages(input_data)
            
            # ä» LLM è·å– token æµ
            print("ğŸ¤– [DEBUG] è°ƒç”¨æ™®é€š LLM èŠå¤©æ¥å£...")
            token_stream = chat_func(messages, self._system)
            complete_response = ""
            
            async for token in token_stream:
                yield token
                complete_response += token
            
            # å­˜å‚¨å®Œæ•´å“åº”
            print(f"âœ… [DEBUG] æ™®é€šèŠå¤©å®Œæˆï¼Œå“åº”é•¿åº¦: {len(complete_response)}")
            self._add_message(complete_response, "assistant")
        
        return chat_with_memory

    def _to_messages(self, input_data: BatchInput) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡æ”¯æŒå›¾åƒçš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = self._memory.copy()
        
        if input_data.images:
            content = []
            text_content = self._to_text_prompt(input_data)
            content.append({"type": "text", "text": text_content})
            
            for img_data in input_data.images:
                content.append({
                    "type": "image_url",
                    "image_url": {"url": img_data.data, "detail": "auto"},
                })
            
            user_message = {"role": "user", "content": content}
        else:
            user_message = {"role": "user", "content": self._to_text_prompt(input_data)}
        
        messages.append(user_message)
        self._add_message(user_message["content"], "user")
        return messages

    async def chat(self, input_data: BatchInput) -> AsyncIterator[SentenceOutput]:
        """èŠå¤©æ–¹æ³•"""
        chat_func = self._create_chat_function()
        async for output in chat_func(input_data):
            yield output